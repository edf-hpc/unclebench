<?xml version="1.0" encoding="utf-8"?>
<jube>

    <include-path>
	<path> $UBENCH_PLATFORM_DIR </path>
    </include-path>
    <include from="platforms.xml" path="include-path"/>

    <multisource>
      <source protocol="https">
        <url>http://icl.cs.utk.edu/projectsfiles/hpcc/download/</url>
        <file>hpcc-1.4.3.tar.gz</file>
      </source>

      <source protocol="local" >
	<file>$UBENCH_BENCHMARK_DIR/hpcc/Make.gnu</file>
	<file>$UBENCH_BENCHMARK_DIR/hpcc/Make.intel</file>
	<file>$UBENCH_BENCHMARK_DIR/hpcc/hpccinf.txt.in</file>
	<file>$UBENCH_BENCHMARK_DIR/hpcc/io.c</file>
    </source>

    </multisource>
    
    <benchmark name="HPCC" outpath="benchmark_runs">
	<comment>HPCC benchmarks</comment>
	
	<fileset name="source">
	    <link> $UBENCH_RESOURCE_DIR/hpcc/hpcc-1.4.3.tar.gz</link>
	    <link> $UBENCH_RESOURCE_DIR/hpcc/hpccinf.txt.in</link>
	    <link> $UBENCH_RESOURCE_DIR/hpcc/Make.gnu</link>
	    <link> $UBENCH_RESOURCE_DIR/hpcc/Make.intel</link>
	    <link> $UBENCH_RESOURCE_DIR/hpcc/io.c</link>
	    <prepare>tar -xzf hpcc-1.4.3.tar.gz </prepare>
	</fileset>
	
	<parameterset name="variant_set">
	    <parameter name="variant_v">0</parameter>
	    <parameter name="variant_name" mode="python">
		["Full_MPI"][$variant_v]
	    </parameter>
	    
 	    <parameter name="variant_NB">
		192
	    </parameter>
	    
	    <parameter name="memory_proportion">
		0.1
	    </parameter>

	    <!-- Choose N to avoid exceeding memory_proportion of available memory -->
	    <parameter name="variant_Ntemp" mode="python" type="float" separator="??" >
		(${memory_proportion}*${nodes}*(${GB_per_node})*1e9/8) ** 0.5		
	    </parameter>
	    
 	    <parameter name="variant_N" mode="python" type="int" separator="??" >
		int( round( ${variant_Ntemp} / ${variant_NB} ) * ${variant_NB})
	    </parameter>
	    
	</parameterset>

	<parameterset name="compiler_opts">
	  <parameter name="arch" mode="python">
	    ["gnu","intel"][$comp_v]
	  </parameter>
	</parameterset>
	
	<!-- =====================  Compile  ===================== -->
	<step name="compile" export="true">
	    	    
	    <!-- Choose compiler and MPI versions -->
	    <use from="platform.xml"> compiler_set </use>
	    <use from="platform.xml"> mpi_set </use>
	    <use> compiler_opts </use>
	    
	    <!-- substitute compiler in makefile -->
	    <use>source</use>
	    
	    <!-- Load environment -->
	    <do> module purge </do>
	    <do> module load $module_compile $module_mpi $module_blas </do>

	    <do work_dir="hpcc-1.4.3">
	        if [[ $MKLROOT ]]; then
	          cp -rf ${blas_root}/mkl/interfaces/fftw2x_cdft .
	      	  cp -rf ${blas_root}/mkl/interfaces/fftw2xc .
	          mkdir fftwmkl
		  cd fftw2x_cdft
		  if [$I_MPI_ROOT]; then
	            make -s libintel64 MKLROOT=${blas_root}/mkl mpi=intelmpi PRECISION=MKL_DOUBLE interface=ilp64 INSTALL_DIR=../fftwmkl;
		  else
		    make -s libintel64 MKLROOT=${blas_root}/mkl mpi=openmpi PRECISION=MKL_DOUBLE interface=ilp64 INSTALL_DIR=../fftwmkl;
		  fi
		  cd ..
		  cd fftw2xc
		  if [[ $I_MPI_ROOT ]]; then
	            make -s libintel64 MKLROOT=${blas_root}/mkl mpi=intelmpi PRECISION=MKL_DOUBLE interface=ilp64 INSTALL_DIR=../fftwmkl;
		  else
		    make -s libintel64 MKLROOT=${blas_root}/mkl mpi=openmpi PRECISION=MKL_DOUBLE interface=ilp64 INSTALL_DIR=../fftwmkl;
		  fi
		  cd ..
		  export LD_LIBRARY_PATH=$${PWD}/fftwmkl:$LD_LIBRARY_PATH;
		fi

		export LD_LIBRARY_PATH=${blas_root}/lib:$LD_LIBRARY_PATH;
		
		cp ../Make.* hpl/.
		cp ../io.c src/.
		make arch=${arch} FFTWROOT="$${PWD}/fftwmkl" CC=${mpi_cc} BLAS_ROOT=${blas_root} MPdir=${mpi_root}
	    </do>

	</step>

	<!-- ====================  Execute  ===================== -->
	
	<fileset name="binaries">
	    <link rel_path_ref="internal" directory="compile/hpcc-1.4.3">
		hpcc
	    </link>
	    <link>pq_script.py</link>
	</fileset>

	<substituteset name="sub_hpcc_parameters">
	  <iofile in="compile/hpccinf.txt.in" out="hpccinf.txt"/>
	  <sub source="#PBSIZE#" dest="1"/>
	  <sub source="#NNS#" dest="$variant_N"/>
	  <sub source="#NNBS#" dest="1"/>
	  <sub source="#NBS#" dest="$variant_NB"/>
	</substituteset> 

	<parameterset name="system_parameters" init_with="platform.xml">
	    <parameter name="nodes" type="int">1</parameter>
	    <parameter name="taskspernode" mode="python" type="int">$NUMA_regions*$cores_per_NUMA_region</parameter>
	    <parameter name="threadspertask" type="int">1</parameter>
	    <parameter name="executable">./hpcc</parameter>
	    <parameter name="modules">$module_compile $module_mpi $module_blas</parameter>
	    <parameter name="timelimit">08:00:00</parameter>
	</parameterset>

	<parameterset name="execute_set" init_with="platform.xml">

	    <parameter name="args_starter" separator="??">
		${binding_full_node}
	    </parameter>
	    
	</parameterset>
	
	<step name="execute" depend="compile">
	  
	  <use from="platform.xml">cluster_specs</use>
	  <use>binaries</use>
	  <use>sub_hpcc_parameters</use>

	  <use>execute_set</use>
	  <use>system_parameters</use>
	  <use>variant_set</use>

	 

	  <use from="platform.xml">jobfiles</use>
	  <use from="platform.xml">execute_sub</use>

	  <do> ./pq_script.py $tasks </do>
	  
	  <do done_file="$done_file">$submit $submit_script</do>
	 	  
	</step>
	
	<!-- =====================  Analyze  ===================== -->
	
	<patternset name="pattern">
	    <pattern name="MPIRandAcc_GUPs" type="float">
	      ^MPIRandomAccess_GUPs=$jube_pat_fp
	    </pattern>
	    
	    <pattern name="MPIFFT_Gflops" type="float">
	      ^MPIFFT_Gflops=$jube_pat_fp
	    </pattern>
	    
	    <pattern name="PTRANS_GBs" type="float">
	      ^PTRANS_GBs=$jube_pat_fp
	    </pattern>
	    
	    <pattern name="StarDGEMM_Gflops" type="float">
	      ^StarDGEMM_Gflops=*$jube_pat_fp
	    </pattern>

	    <pattern name="RORingBandwidth_GBytes" type="float">
	      ^RandomlyOrderedRingBandwidth_GBytes=$jube_pat_fp
	    </pattern>

	    <pattern name="RORingLatency_usec" type="float">
	      ^RandomlyOrderedRingLatency_usec=$jube_pat_fp
	    </pattern>

	    <pattern name="StarSTREAM_Triad" type="float">
	      ^StarSTREAM_Triad=$jube_pat_fp
	    </pattern>

	</patternset>
	

	<analyzer name="analyse">
	  <use>pattern</use>
	  <analyse step="execute"><file>hpccoutf.txt</file></analyse>
	</analyzer>

	<!-- =====================  Result  ===================== -->

	<result>
	    <use>analyse</use>
	    <table name="result" style="csv" sort="nodes">
		<column>nodes</column>
		<column>MPIRandAcc_GUPs</column>
		<column>MPIFFT_Gflops</column>
		<column>PTRANS_GBs</column>
		<column>StarDGEMM_Gflops</column>
		<column>RORingBandwidth_GBytes</column>
		<column>RORingLatency_usec</column>
		<column>StarSTREAM_Triad</column>
		<column>modules</column>
	    </table>
	</result>
		
    </benchmark>
</jube>
