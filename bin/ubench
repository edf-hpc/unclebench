#!/usr/bin/env python
# -*- coding: utf-8 -*-
##############################################################################
#  This file is part of the UncleBench benchmarking tool.                    #
#        Copyright (C) 2019 EDF SA                                           #
#                                                                            #
#  UncleBench is free software: you can redistribute it and/or modify        #
#  it under the terms of the GNU General Public License as published by      #
#  the Free Software Foundation, either version 3 of the License, or         #
#  (at your option) any later version.                                       #
#                                                                            #
#  UncleBench is distributed in the hope that it will be useful,             #
#  but WITHOUT ANY WARRANTY; without even the implied warranty of            #
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the              #
#  GNU General Public License for more details.                              #
#                                                                            #
#  You should have received a copy of the GNU General Public License         #
#  along with UncleBench. If not, see <http://www.gnu.org/licenses/>.        #
#                                                                            #
##############################################################################
# pylint: disable=invalid-name
""" Unclebench main script """


import os
import sys
import argparse
import socket

import ubench.core.ubench_commands as ubench_commands  # pylint: disable=no-name-in-module
import ubench.core.ubench_config as uconfig  # pylint: disable=no-name-in-module
from ubench.release import __version__  # pylint: disable=no-name-in-module


os.environ['JUBE_EXEC_SHELL'] = '/bin/bash'
uconf = uconfig.UbenchConfig()  # Set default paths for all directories
platform_list = uconf.get_platform_list()
benchmark_list = uconf.get_benchmark_list()
default_platform = None
platform_required = True

for platform_name in platform_list:
    if platform_name in socket.gethostname().lower():
        default_platform = platform_name
        platform_required = False

# Build an argparse with a subparse for each main ubench option
parser = argparse.ArgumentParser(description='Unclebench benchmarking and reporting tool.',
                                 formatter_class=argparse.RawTextHelpFormatter)

parser.add_argument('-v',
                    '--version',
                    action='version',
                    version='%(prog)s {version}'.format(version=__version__))

subparsers = parser.add_subparsers(dest='subparser_name')

# Parser for fetch
fetch_help = ('Downloads benchmark sources and test cases'
              ' from benchmark defined location to /scratch/<user>/Ubench/resource'
              ' (customizable with UBENCH_RESOURCE_DIR environment variable).')

parser_fetch = subparsers.add_parser('fetch',
                                     help=fetch_help)

parser_fetch.add_argument('-b',
                          help=('Benchmarks to fetch. Benchmark definition'
                                ' files should be located in /usr/share/unclebench/benchmarks/'
                                ' (can be custumosized with UBENCH_BENCHMARK_DIR environment'
                                ' variable)'),
                          nargs='+',
                          choices=benchmark_list,
                          required=True)

# Parser for run
run_help = ('Execute benchmarks in /scratch/<user>/Ubench/benchmarks directory '
            '(can be customized with UBENCH_RUN_DIR_BENCH directory)')

parser_run = subparsers.add_parser('run', help=run_help)

parser_run.add_argument('-p',
                        help=('Name of the test platform.'
                              ' Platform definition files should be located'
                              ' in/usr/share/unclebench/platform/'
                              ' (can be customized with UBENCH_PLATFORM_DIR environment variable)'),
                        required=platform_required,
                        choices=platform_list,
                        default=default_platform)

parser_run.add_argument('-b',
                        help=('Benchmarks to run.'
                              ' Benchmark definition files should be located in'
                              ' /usr/share/unclebench/benchmarks/'
                              ' (can be custumosized with UBENCH_BENCHMARK_DIR environment'
                              ' variable)'),
                        nargs='+',
                        required=True,
                        choices=benchmark_list)

parser_run.add_argument('-w',
                        help=('Nodes on which to run benchmarks'
                              ' ex: -w 4,pocn[380,431-433]. You can also launch job'
                              ' on all idle nodes'
                              ' ex: -w all4 to run a benchmark with 4 nodes jobs covering'
                              ' every idle node'),
                        nargs='+')

parser_run.add_argument('-c',
                        '--custom-params',
                        help=('Set custom parameters. ex : --custom-params mpiv:0.'
                              ' Use ubench listparams to know which parameters are customizable'),
                        nargs='+',
                        default=[])

parser_run.add_argument('-f',
                        '--file-params',
                        help='File to set custom parameters')

parser_run.add_argument('-e',
                        '--execute',
                        help='Peforms only the execution step of a benchmark',
                        default=False,
                        action='store_true')

parser_run.add_argument('--foreground',
                        help='Peforms only the execution step of a benchmark',
                        default=False,
                        action='store_true')


## campaign command
# ubench campaign -f
parser_campaign = subparsers.add_parser('campaign', help=('Run benchmark campaigns'))
parser_campaign.add_argument('-f',
                             '--campaign_file',
                             help='File which describes benchmark campaign',
                             required=True)

parser_campaign.add_argument('-r',
                        '--reference',
                        help=('Result referecence: '
                              'either a Git tag or Git commit hash'))


# Parser list
list_help = 'List existing runs information for a given benchmark.'

parser_list = subparsers.add_parser('list',
                                    help=list_help)

parser_list.add_argument('-p',
                         help='Name of the test platform',
                         required=platform_required,
                         choices=platform_list,
                         default=default_platform)

parser_list.add_argument('-b',
                         help='Benchmark names list',
                         required=True,
                         nargs='+',
                         choices=benchmark_list)

# Parser log
parser_log = subparsers.add_parser('log',
                                   help='Print log of a benchmark run given its ID.')

parser_log.add_argument('-p',
                        help='Name of the test platform',
                        required=platform_required,
                        choices=platform_list,
                        default=default_platform)

parser_log.add_argument('-b',
                        help='Benchmark names list',
                        required=True,
                        nargs='+',
                        choices=benchmark_list)

parser_log.add_argument('-i',
                        help='Benchmark run IDs',
                        nargs='+')

# Parser listparams
parser_listparams = subparsers.add_parser('listparams',
                                          help='List customizable parameters of a benchmark.')

parser_listparams.add_argument('-p',
                               help='Name of the test platform',
                               required=platform_required,
                               choices=platform_list,
                               default=default_platform)

parser_listparams.add_argument('-b',
                               help='Benchmark names list',
                               required=True,
                               nargs='+',
                               choices=benchmark_list)

parser_listparams.add_argument('-d',
                               help='Print default parameters values',
                               default=False,
                               action='store_true')

# Parser result
parser_result = subparsers.add_parser('result',
                                      help=('Print raw results array from a benchmark run.'
                                            ' To be found benchmark runs must be located in'
                                            ' /sratch/<user>/Ubench/benchmarks directory but'
                                            ' this path can be customized with'
                                            ' UBENCH_RUN_DIR_BENCH environment variable'))

parser_result.add_argument('-p',
                           help='Name of the test platform',
                           required=platform_required,
                           choices=platform_list,
                           default=default_platform)

parser_result.add_argument('-b',
                           help='Benchmark names list',
                           required=True,
                           nargs='+',
                           choices=benchmark_list)

parser_result.add_argument('-i',
                           help='Benchmark run IDs',
                           nargs='+')

# Parser report
report_help = 'Build a performance report from benchmark result directories.'

parser_report = subparsers.add_parser('report',
                                      help=report_help)

parser_report.add_argument('-m',
                           '--metadata-file',
                           help='Metadata file containing everything needed to build the report',
                           required=True)

parser_report.add_argument('-o',
                           '--output-dir',
                           help='Write report files in OUTPUT_DIR',
                           required=True)

# Parser compare
parser_compare = subparsers.add_parser('compare',
                                       help='Compare results from different run directories.')

parser_compare.add_argument('-i',
                            '--input-dirs',
                            nargs='+',
                            help='Directories where results are to compared',
                            required=True)

parser_compare.add_argument('-b',
                            '--benchmark-name',
                            help='Name of the benchmark from which results are compared',
                            required=True)

parser_compare.add_argument('-c',
                            '--context',
                            default=None,
                            help=('Fields to use as context and should not'
                                  ' be compared but merged in the result table'),
                            required=False,
                            nargs='+')

parser_compare.add_argument('-cc',
                            '--compared-context',
                            default=None,
                            help='Field to use as context and whose value should be compared',
                            required=False)

parser_compare.add_argument('-t',
                            '--threshold',
                            default=None,
                            help='Differences under given threshold will not be printed')

# Parser info
parser_info = subparsers.add_parser('info',
                                    help='Displays path settings origin.')

args = parser.parse_args()

platform_arg = ''
benchmark_arg = ''
if 'p' in vars(args):
    platform_arg = args.p
if 'b' in vars(args):
    benchmark_arg = args.b

commands = ubench_commands.UbenchCmd(platform=platform_arg, benchmark_list=benchmark_arg)

if args.subparser_name == 'run':
    options_dictionary = vars(args)
    options_dictionary['raw_cli'] = sys.argv
    sys.exit(not commands.run(options_dictionary))
elif args.subparser_name == 'campaign':
    commands.campaign(args.campaign_file, args.reference)
elif args.subparser_name == 'fetch':
    commands.fetch()
elif args.subparser_name == 'result':
    commands.result(args.i)
elif args.subparser_name == 'log':
    commands.log(id_list=args.i)
elif args.subparser_name == 'list':
    commands.listb()
elif args.subparser_name == 'listparams':
    commands.list_parameters(default_values=args.d)
elif args.subparser_name == 'report':
    commands.report(args.metadata_file, args.output_dir)
elif args.subparser_name == 'compare':
    commands.compare(args.input_dirs, args.benchmark_name,
                     (args.context, args.compared_context), args.threshold)
elif args.subparser_name == 'info':
    uconf.print_config()
